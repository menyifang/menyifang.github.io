<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="language" content="english">
<title>DCT-Net</title>
<meta name="description" content="DCT-Net: Domain-Calibrated Translation for Portrait Stylization">
<meta name="author" content="Yifang Men">
<link rel="icon" type="image/x-icon" href="http://www.icst.pku.edu.cn/favicon.ico">
<link rel="stylesheet" type="text/css" href="./DCTNet_files/project.css">
</head>

<body>
<div id="main">
  
	<div class="content"><br>
		<h1>DCT-Net: Domain-Calibrated Translation for Portrait Stylization </h1>
		<div class="authors">
			<div class="author">
				 <a href="https://menyifang.github.io/" style="text-decoration: none">Yifang Men</a>
		                </div>

                                                <div class="author">
				 <a href="mailto:yaoy92@gmail.com" style="text-decoration: none">Yuan Yao</a>
			</div>

                                                <div class="author">
				 <a href="mailto:miaomiao.cmm@alibaba-inc.com" style="text-decoration: none">Miaomiao Cui</a>
			</div>
                                                <div class="author">
				 <a href="http://www.icst.pku.edu.cn/zlian/" style="text-decoration: none">Zhouhui Lian</a>
			</div>
		                <div class="author">
				 <a href="https://scholar.google.com/citations?user=M0Ei1zkAAAAJ&hl=en" style="text-decoration: none">Xuansong Xie</a>
			</div>
			
	
		</div>
		<br>
	  	<p class="banner" align="center"><em>Accepted by SIGGRAPH 2022, Journal Track. </em></p>
		<div class="overview sec">
			<div class="picture_wrapper">
                <img src="./DCTNet_files/teaser.gif" width="100%" alt="Teaser">
		  		
	  		</div>
	  	</div>


        
        <div class="Abstract sec">
            <h2>Abstract</h2>
            <div class="desp">
                <p style="text-align:justify">
                This paper introduces DCT-Net, a novel image translation architecture for few-shot portrait stylization. Given limited style exemplars (âˆ¼100), the new architecture can produce high-quality style transfer results with advanced ability to synthesize high-fidelity contents and strong generality to handle complicated scenes (e.g., occlusions and accessories). Moreover, it enables full-body image translation via one elegant evaluation network trained by partial observations (i.e., stylized heads). Few-shot learning based style transfer is challenging since the learned model can easily become overfitted in the target domain, due to the biased distribution formed by only a few training examples. This paper aims to handle the challenge by adopting the key idea of "calibration first, translation later" and exploring the augmented global structure with locally-focused translation. Specifically, the proposed DCT-Net consists of three modules: a content adapter borrowing the powerful prior from source photos to calibrate the content distribution of target samples; a geometry expansion module using affine transformations to release spatially semantic constraints; and a texture translation module leveraging samples produced by the calibrated distribution to learn a fine-grained conversion. Experimental results demonstrate the proposed method's superiority over the state of the art in head stylization and its effectiveness on full image translation with adaptive deformations.
</p>
            </div>
                    </div>



                <div class="overview sec">
            <h2>Core idea</h2>
            <div class="picture_wrapper2" align="center">

                  <img src="./DCTNet_files/overview.png" width="50%" alt="Teaser">
                  <p style="text-align: center">Fig.2 An illustration of domain-calibrated translation. </p>
              </div>
          </div>


                <div class="network sec">
            <h2>Networks</h2>
            <div class="picture_wrapper" align="center">

                  <img src="./DCTNet_files/network.png" width="50%" alt="Teaser">
                  <p style="text-align: center">Fig.3 An overview of the proposed framework. </p>
              </div>
          </div>






		<div class="download sec">
			<h2>Download</h2>
			<div>
				<li><strong>Paper</strong>: <a href="https://arxiv.org/abs/2207.02426">arxiv</a></li>
                <li><strong>Code</strong>: <a href="https://github.com/menyifang/DCT-Net">github</a></li>
                <li><strong>Supplemental materials</strong>: <a href="./DCTNet_files/Supp_DCTNet_TOG2022.pdf">pdf</a></li>
                <li><strong>Video</strong>: <a href="https://www.youtube.com/watch?v=Y8BrfOjXYQM">mp4</a></li>
			</div>
		</div>
        

        <div class="Supplemental Video sec">
            <h2>Demo Video</h2>
<div align="center">
<iframe width="720" height="405" src="https://www.youtube.com/embed/Y8BrfOjXYQM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
</div>
        



       <div class="citation sec">
			<h2>Citation</h2>
			<p class="bibtex">@inproceedings{men2022dct,
&nbsp; title={DCT-Net: Domain-Calibrated Translation for Portrait Stylization},
&nbsp; author={Men, Yifang and Yao, Yuan and Cui, Miaomiao and Lian, Zhouhui and Xie, Xuansong},
&nbsp; journal={ACM Transactions on Graphics (TOG)},
&nbsp; volume={41},
&nbsp; number={4},
&nbsp; pages={1--9},
&nbsp; year={2022},
&nbsp; publisher={ACM New York, NY, USA}
}
			</p>
		</div>


<div align="center">
<a href="https://menyifang.github.io/projects">=&gt; Back to main project page &lt;=</a>
</div>

  </div>
</div>


</body></html>
